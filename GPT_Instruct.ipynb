{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT Instruct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    Trainer,\n",
    "    TrainingArguments\n",
    ")\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make a function to create examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(example):\n",
    "    example[\"prompt\"] = f\"{example['instruction']} {example['input']} {example['output']}\"\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a function to tokenize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(dataset):\n",
    "    tokenized_dataset = dataset.map(lambda example: tokenizer(example['prompt'], truncation=True, max_length=128), batched=True, remove_columns=['prompt'])\n",
    "\n",
    "    return tokenized_dataset\n",
    "\n",
    "# Limited to 128 tokens to reduce processing resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              instruction  \\\n",
      "200130  What is the value of the math expression 100(l...   \n",
      "490147  What should I do to get ready for a job interv...   \n",
      "194683  Create a function that returns the `n`th numbe...   \n",
      "238015  What is the class of the word \"training\" in th...   \n",
      "71162   Identify the emotion expressed by the writer i...   \n",
      "319977  Analyze the given dataset and propose a suitab...   \n",
      "245136  Which of the following words are considered a ...   \n",
      "76590   What would be the title of a book about my lif...   \n",
      "150443         Explain the rules of Sudoku, step by step.   \n",
      "450799                how to Compile Python to Javascript   \n",
      "89365   Classify whether the claim is supported by evi...   \n",
      "184183                 What are today's top news stories.   \n",
      "277659  Edit the following sentence: \"I took my dog fo...   \n",
      "25681     How would you measure the success of a website?   \n",
      "440892                         What is FASTQ file format?   \n",
      "465182  why we imagine god with white long hair and beard   \n",
      "356139  Write down 3 things you would do if you had a ...   \n",
      "197069  Write a Java code snippet to print out the cur...   \n",
      "12052   Write two sentences that describe the painting...   \n",
      "126670  Find a new subreddit for the user.\\n\\nI am int...   \n",
      "\n",
      "                                                   output  \\\n",
      "200130                                                100   \n",
      "490147  To prepare for a job interview, it is importan...   \n",
      "194683  def fibonacci(n):\\n    if n <= 1:\\n        ret...   \n",
      "238015                                          Noun, NN.   \n",
      "71162                                            surprise   \n",
      "319977  Based on the dataset provided, a targeted mark...   \n",
      "245136         deified, kayak, wow, peep, noon, abba, bob   \n",
      "76590                            The Life of a Programmer   \n",
      "150443  1. Sudoku is a logic-based number-placement pu...   \n",
      "450799  Compiling Python to JavaScript involves transl...   \n",
      "89365                                           Supported   \n",
      "184183  Today's top news stories include the COVID-19 ...   \n",
      "277659                       I took my dog out for a run.   \n",
      "25681   The success of a website can be measured by me...   \n",
      "440892  FASTQ is a file format used to store high-thro...   \n",
      "465182  The depiction of God with white hair and a bea...   \n",
      "356139  1. Travel the world\\n2. Buy a house\\n3. Start ...   \n",
      "197069  long currentTimeMillis = System.currentTimeMil...   \n",
      "12052   Sandro Botticelli's painting \"The Birth of Ven...   \n",
      "126670                                 /r/machinelearning   \n",
      "\n",
      "                                                    input  \n",
      "200130                                                     \n",
      "490147                                                     \n",
      "194683                                                     \n",
      "238015                                                     \n",
      "71162                                                      \n",
      "319977  Dataset: Age group with highest spending: 25-3...  \n",
      "245136                                                     \n",
      "76590                                                      \n",
      "150443                                                     \n",
      "450799                                                     \n",
      "89365                                                      \n",
      "184183                                                     \n",
      "277659                           I took my dog for a run.  \n",
      "25681                                                      \n",
      "440892                                                     \n",
      "465182                                                     \n",
      "356139                                                     \n",
      "197069                                                     \n",
      "12052                                                      \n",
      "126670                                                     \n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"hakurei/open-instruct-v1\", split=\"train\")\n",
    "# 楽園の素敵な巫女\n",
    "print(dataset.to_pandas().sample(20))\n",
    "# a dataset of instructions and responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(preprocess, remove_columns=[\"instruction\", \"input\", \"output\"])\n",
    "\n",
    "dataset = dataset.shuffle(seed=42).select(range(10000)).train_test_split(test_size=0.1)\n",
    "# limiting the training data to 10k entries to save processing time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the training/test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 9000/9000 [00:02<00:00, 3728.36 examples/s]\n",
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 4362.34 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "MODEL_NAME = \"microsoft/DialoGPT-medium\"\n",
    "# This model is a variant of GPT2 that has some training instructions, but not enough. The goal will be to fine tune and improve it.\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer. eos_token\n",
    "# this is necessary to keep the questions and responses separate (I don't completly understand why)\n",
    "\n",
    "train_dataset = tokenize_dataset(train_dataset)\n",
    "test_dataset = tokenize_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "# creates the appropriate batches\n",
    "\n",
    "traing_args = TrainingArguments(output_dir=\"models/dialo_gpt\",\n",
    "                                num_train_epochs=1,\n",
    "                                per_device_train_batch_size=8,\n",
    "                                per_device_eval_batch_size=16)\\\n",
    "# One epoch and modest batch sizes to reduce processing time (defaults are 10, 32, 32)\n",
    "trainer = Trainer(model=model,\n",
    "                    args=traing_args,\n",
    "                    train_dataset=train_dataset,\n",
    "                    eval_dataset=test_dataset,\n",
    "                    data_collator=data_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = traing_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "trainer.save_model()\n",
    "\n",
    "prompt = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A function to return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt):\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt').to(\"cuda\")    # use a CUDA GPU if possible\n",
    "    outputs = model.generate(inputs, max_length=64, pad_token_id=tokenizer.eos_token_id)\n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return generated[:generated.rfind('.')+1] # remove the last sentence from the output (don't know why)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text(\"What's the best way to cook chiken breast?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
