{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, AutoTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# the tutorial used scipi's cosine function but that didn't work, and Bing suggested this instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model and the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-cased\"\n",
    "\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Tokenize me this please\"\n",
    "encoded_inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "# wrapped in a tensor object\n",
    "output = model(**encoded_inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the last hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_state = output.last_hidden_state\n",
    "# the final state before ending customizations\n",
    "pooler_output = output.pooler_output\n",
    "\n",
    "print(last_hidden_state.shape)\n",
    "# this represents the vector space for each token\n",
    "print(pooler_output.shape)\n",
    "# this represents a new vector space for the entire sentence. This is useful for sentiment classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A function to show that the same token can get a unique, context-specific encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    encoded_inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    return model(**encoded_inputs)[0]\n",
    "\n",
    "sentence1 = \"There was a fly drinking from my soup\"\n",
    "sentence2 = \"To become a commercial pikot, you have to fly for 1500 hours\"\n",
    "\n",
    "tokens1 = tokenizer.tokenize(sentence1)\n",
    "tokens2 = tokenizer.tokenize(sentence2)\n",
    "\n",
    "\n",
    "out1 = predict(sentence1)\n",
    "out2 = predict(sentence2)\n",
    "\n",
    "emb1 = out1[0:, tokens1.index(\"fly\"), :].detach()\n",
    "emb2 = out2[0:, tokens2.index(\"fly\"), :].detach()\n",
    "\n",
    "print(emb1.shape)\n",
    "print(emb2.shape)\n",
    "\n",
    "print(emb1)\n",
    "print(emb2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the cosine distance between the two vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb1 = emb1.reshape(-1, emb1.shape[-1])\n",
    "emb2 = emb2.reshape(-1, emb2.shape[-1])\n",
    "# this wasn't necessary in the tutorial, but it was here. I don't know why.\n",
    "\n",
    "print(cosine_similarity(emb1, emb2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
